{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TLEtoAC/Prompt-Classification/blob/main/SLM_promptquality.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pgrep -a ollama\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KYs1xsE9sJF",
        "outputId": "56b33936-454d-4785-b4d8-6161176a10ed"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41370 ollama serve\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull phi3:mini\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yT_ojQ4J-set",
        "outputId": "31607a25-f9fe-4958-e955-541422fc1a5e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama run phi3:mini \"Explain overfitting in machine learning\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_ZLeFdk-wXN",
        "outputId": "82e74f0f-0c17-458b-f4c3-ed2c0ef4640b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25hOver\u001b[?25l\u001b[?25hf\u001b[?25l\u001b[?25hitting\u001b[?25l\u001b[?25h is\u001b[?25l\u001b[?25h a\u001b[?25l\u001b[?25h common\u001b[?25l\u001b[?25h problem\u001b[?25l\u001b[?25h that\u001b[?25l\u001b[?25h occurs\u001b[?25l\u001b[?25h in\u001b[?25l\u001b[?25h machine\u001b[?25l\u001b[?25h learning\u001b[?25l\u001b[?25h when\u001b[?25l\u001b[?25h a\u001b[?25l\u001b[?25h model\u001b[?25l\u001b[?25h lear\u001b[?25l\u001b[?25hns\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h training\u001b[?25l\u001b[?25h data\u001b[?25l\u001b[?25h too\u001b[?25l\u001b[?25h well\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h including\u001b[?25l\u001b[?25h its\u001b[?25l\u001b[?25h noise\u001b[?25l\u001b[?25h and\u001b[?25l\u001b[?25h out\u001b[?25l\u001b[?25hliers\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25h It\u001b[?25l\u001b[?25h essentially\u001b[?25l\u001b[?25h means\u001b[?25l\u001b[?25h that\u001b[?25l\u001b[?25h while\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h model\u001b[?25l\u001b[?25h performs\u001b[?25l\u001b[?25h exception\u001b[?25l\u001b[?25hally\u001b[?25l\u001b[?25h on\u001b[?25l\u001b[?25h this\u001b[?25l\u001b[?25h dataset\u001b[?25l\u001b[?25h due\u001b[?25l\u001b[?25h to\u001b[?25l\u001b[?25h these\u001b[?25l\u001b[?25h imper\u001b[?25l\u001b[?25hfe\u001b[?25l\u001b[?25hctions\u001b[?25l\u001b[?25h being\u001b[?25l\u001b[?25h learned\u001b[?25l\u001b[?25h as\u001b[?25l\u001b[?25h part\u001b[?25l\u001b[?25h of\u001b[?25l\u001b[?25h patterns\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h it\u001b[?25l\u001b[?25h fails\u001b[?25l\u001b[?25h significantly\u001b[?25l\u001b[?25h at\u001b[?25l\u001b[?25h predict\u001b[?25l\u001b[?25hing\u001b[?25l\u001b[?25h or\u001b[?25l\u001b[?25h general\u001b[?25l\u001b[?25hizing\u001b[?25l\u001b[?25h results\u001b[?25l\u001b[?25h for\u001b[?25l\u001b[?25h new\u001b[?25l\u001b[?25h un\u001b[?25l\u001b[?25hseen\u001b[?25l\u001b[?25h datasets\u001b[?25l\u001b[?25h because\u001b[?25l\u001b[?25h such\u001b[?25l\u001b[?25h errors\u001b[?25l\u001b[?25h are\u001b[?25l\u001b[?25h not\u001b[?25l\u001b[?25h representative\u001b[?25l\u001b[?25h features\u001b[?25l\u001b[?25h but\u001b[?25l\u001b[?25h rather\u001b[?25l\u001b[?25h artifact\u001b[?25l\u001b[?25hs\u001b[?25l\u001b[?25h specific\u001b[?25l\u001b[?25h to\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h training\u001b[?25l\u001b[?25h data\u001b[?25l\u001b[?25h itself\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hThe\u001b[?25l\u001b[?25h primary\u001b[?25l\u001b[?25h reason\u001b[?25l\u001b[?25h over\u001b[?25l\u001b[?25hf\u001b[?25l\u001b[?25hitting\u001b[?25l\u001b[?25h occurs\u001b[?25l\u001b[?25h is\u001b[?25l\u001b[?25h that\u001b[?25l\u001b[?25h a\u001b[?25l\u001b[?25h model\u001b[?25l\u001b[?25h lear\u001b[?25l\u001b[?25hns\u001b[?25l\u001b[?25h and\u001b[?25l\u001b[?25h memor\u001b[?25l\u001b[?25hizes\u001b[?25l\u001b[?25h too\u001b[?25l\u001b[?25h much\u001b[?25l\u001b[?25h detail\u001b[?25l\u001b[?25h from\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h training\u001b[?25l\u001b[?25h dataset\u001b[?25l\u001b[?25h without\u001b[?25l\u001b[?25h capt\u001b[?25l\u001b[?25huring\u001b[?25l\u001b[?25h its\u001b[?25l\u001b[?25h underlying\u001b[?25l\u001b[?25h patterns\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h leading\u001b[?25l\u001b[?25h it\u001b[?25l\u001b[?25h to\u001b[?25l\u001b[?25h perform\u001b[?25l\u001b[?25h poor\u001b[?25l\u001b[?25hly\u001b[?25l\u001b[?25h when\u001b[?25l\u001b[?25h presented\u001b[?25l\u001b[?25h with\u001b[?25l\u001b[?25h different\u001b[?25l\u001b[?25h information\u001b[?25l\u001b[?25h during\u001b[?25l\u001b[?25h testing\u001b[?25l\u001b[?25h or\u001b[?25l\u001b[?25h real\u001b[?25l\u001b[?25h-\u001b[?25l\u001b[?25hworld\u001b[?25l\u001b[?25h application\u001b[?25l\u001b[?25h stages\u001b[?25l\u001b[?25h (\u001b[?25l\u001b[?25hi\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25he\u001b[?25l\u001b[?25h.,\u001b[?25l\u001b[?25h validation\u001b[?25l\u001b[?25h/\u001b[?25l\u001b[?25htesting\u001b[?25l\u001b[?25h phase\u001b[?25l\u001b[?25h).\u001b[?25l\u001b[?25h This\u001b[?25l\u001b[?25h situation\u001b[?25l\u001b[?25h often\u001b[?25l\u001b[?25h happens\u001b[?25l\u001b[?25h due\u001b[?25l\u001b[?25h to\u001b[?25l\u001b[?25h highly\u001b[?25l\u001b[?25h complex\u001b[?25l\u001b[?25h models\u001b[?25l\u001b[?25h like\u001b[?25l\u001b[?25h deep\u001b[?25l\u001b[?25h neural\u001b[?25l\u001b[?25h networks\u001b[?25l\u001b[?25h which\u001b[?25l\u001b[?25h are\u001b[?25l\u001b[?25h over\u001b[?25l\u001b[?25hfit\u001b[?25l\u001b[?25h in\u001b[?25l\u001b[?25h nature\u001b[?25l\u001b[?25h because\u001b[?25l\u001b[?25h they\u001b[?25l\u001b[?25h have\u001b[?25l\u001b[?25h too\u001b[?25l\u001b[?25h many\u001b[?25l\u001b[?25h parameters\u001b[?25l\u001b[?25h relative\u001b[?25l\u001b[?25h to\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h number\u001b[?25l\u001b[?25h of\u001b[?25l\u001b[?25h observations\u001b[?25l\u001b[?25h available\u001b[?25l\u001b[?25h for\u001b[?25l\u001b[?25h training\u001b[?25l\u001b[?25h them\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h leading\u001b[?25l\u001b[?25h these\u001b[?25l\u001b[?25h models\u001b[?25l\u001b[?25h'\u001b[?25l\u001b[?25h performance\u001b[?25l\u001b[?25h on\u001b[?25l\u001b[?25h un\u001b[?25l\u001b[?25hseen\u001b[?25l\u001b[?25h data\u001b[?25l\u001b[?25h suffer\u001b[?25l\u001b[?25h significantly\u001b[?25l\u001b[?25h when\u001b[?25l\u001b[?25h compared\u001b[?25l\u001b[?25h with\u001b[?25l\u001b[?25h their\u001b[?25l\u001b[?25h impress\u001b[?25l\u001b[?25hive\u001b[?25l\u001b[?25h results\u001b[?25l\u001b[?25h on\u001b[?25l\u001b[?25h seen\u001b[?25l\u001b[?25h or\u001b[?25l\u001b[?25h fitted\u001b[?25l\u001b[?25h examples\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25h\n",
            "\u001b[?25l\u001b[?25hTo\u001b[?25l\u001b[?25h avoid\u001b[?25l\u001b[?25h this\u001b[?25l\u001b[?25h problem\u001b[?25l\u001b[?25h and\u001b[?25l\u001b[?25h develop\u001b[?25l\u001b[?25h a\u001b[?25l\u001b[?25h model\u001b[?25l\u001b[?25h that\u001b[?25l\u001b[?25h accur\u001b[?25l\u001b[?25hately\u001b[?25l\u001b[?25h capt\u001b[?25l\u001b[?25hures\u001b[?25l\u001b[?25h patterns\u001b[?25l\u001b[?25h without\u001b[?25l\u001b[?25h learning\u001b[?25l\u001b[?25h noise\u001b[?25l\u001b[?25h from\u001b[?25l\u001b[?25h our\u001b[?25l\u001b[?25h dataset\u001b[?25l\u001b[?25h one\u001b[?25l\u001b[?25h can\u001b[?25l\u001b[?25h employ\u001b[?25l\u001b[?25h techniques\u001b[?25l\u001b[?25h such\u001b[?25l\u001b[?25h as\u001b[?25l\u001b[?25h cross\u001b[?25l\u001b[?25h-\u001b[?25l\u001b[?25hvalidation\u001b[?25l\u001b[?25h during\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h development\u001b[?25l\u001b[?25h process\u001b[?25l\u001b[?25h to\u001b[?25l\u001b[?25h estimate\u001b[?25l\u001b[?25h how\u001b[?25l\u001b[?25h well\u001b[?25l\u001b[?25h your\u001b[?25l\u001b[?25h algorithm\u001b[?25l\u001b[?25h is\u001b[?25l\u001b[?25h likely\u001b[?25l\u001b[?25h to\u001b[?25l\u001b[?25h perform\u001b[?25l\u001b[?25h in\u001b[?25l\u001b[?25h practice\u001b[?25l\u001b[?25h,\u001b[?25l\u001b[?25h early\u001b[?25l\u001b[?25h stopping\u001b[?25l\u001b[?25h where\u001b[?25l\u001b[?25h we\u001b[?25l\u001b[?25h stop\u001b[?25l\u001b[?25h training\u001b[?25l\u001b[?25h when\u001b[?25l\u001b[?25h performance\u001b[?25l\u001b[?25h begins\u001b[?25l\u001b[?25h to\u001b[?25l\u001b[?25h deg\u001b[?25l\u001b[?25hrade\u001b[?25l\u001b[?25h on\u001b[?25l\u001b[?25h validation\u001b[?25l\u001b[?25h data\u001b[?25l\u001b[?25h or\u001b[?25l\u001b[?25h drop\u001b[?25l\u001b[?25hout\u001b[?25l\u001b[?25h technique\u001b[?25l\u001b[?25h commonly\u001b[?25l\u001b[?25h used\u001b[?25l\u001b[?25h with\u001b[?25l\u001b[?25h neural\u001b[?25l\u001b[?25h networks\u001b[?25l\u001b[?25h which\u001b[?25l\u001b[?25h involves\u001b[?25l\u001b[?25h randomly\u001b[?25l\u001b[?25h ign\u001b[?25l\u001b[?25horing\u001b[?25l\u001b[?25h some\u001b[?25l\u001b[?25h neur\u001b[?25l\u001b[?25hons\u001b[?25l\u001b[?25h at\u001b[?25l\u001b[?25h each\u001b[?25l\u001b[?25h step\u001b[?25l\u001b[?25h of\u001b[?25l\u001b[?25h training\u001b[?25l\u001b[?25h so\u001b[?25l\u001b[?25h that\u001b[?25l\u001b[?25h they\u001b[?25l\u001b[?25h do\u001b[?25l\u001b[?25h not\u001b[?25l\u001b[?25h become\u001b[?25l\u001b[?25h too\u001b[?25l\u001b[?25h dependent\u001b[?25l\u001b[?25h on\u001b[?25l\u001b[?25h any\u001b[?25l\u001b[?25h one\u001b[?25l\u001b[?25h input\u001b[?25l\u001b[?25h during\u001b[?25l\u001b[?25h the\u001b[?25l\u001b[?25h learning\u001b[?25l\u001b[?25h process\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25h\n",
            "\n",
            "\u001b[?25l\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "import sys\n",
        "import re\n",
        "\n",
        "# ===================================================================\n",
        "# 1. THE CLASSIFIER (The \"Brain\")\n",
        "# This is the 'phi3:mini' function we built.\n",
        "# ===================================================================\n",
        "\n",
        "# Define the \"brain\" for the classifier.\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a prompt classification router. Your only job is to analyze the user's\n",
        "prompt and respond with a *single word*: low, medium, or high.\n",
        "Do not explain your choice. Do not add any other text.\n",
        "\"\"\"\n",
        "\n",
        "# Create our \"smarter\" few-shot examples.\n",
        "FEW_SHOT_EXAMPLES = [\n",
        "    {'role': 'user', 'content': \"What is 1 + 1?\"},\n",
        "    {'role': 'assistant', 'content': 'low'},\n",
        "    {'role': 'user', 'content': \"who was the president in 1999?\"},\n",
        "    {'role': 'assistant', 'content': 'low'},\n",
        "    {'role': 'user', 'content': \"what is the capital of france\"},\n",
        "    {'role': 'assistant', 'content': 'low'},\n",
        "    {'role': 'user', 'content': \"Why is microservices significant?\"},\n",
        "    {'role': 'assistant', 'content': 'medium'},\n",
        "    {'role': 'user', 'content': \"Explain quantum entanglement in simple terms.\"},\n",
        "    {'role': 'assistant', 'content': 'medium'},\n",
        "    {'role': 'user', 'content': \"List the top 5 programming languages for machine learning.\"},\n",
        "    {'role': 'assistant', 'content': 'medium'},\n",
        "    {'role': 'user', 'content': \"Summarize the following research paper abstract...\"},\n",
        "    {'role': 'assistant', 'content': 'high'},\n",
        "    {'role': 'user', 'content': \"Write a short story about a sad robot.\"},\n",
        "    {'role': 'assistant', 'content': 'high'},\n",
        "]\n",
        "\n",
        "\n",
        "def predict_complexity(prompt_to_classify: str) -> str:\n",
        "    \"\"\"\n",
        "    Uses the 'phi3:mini' model to classify a new prompt.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {'role': 'system', 'content': SYSTEM_PROMPT}\n",
        "    ]\n",
        "    messages.extend(FEW_SHOT_EXAMPLES)\n",
        "    messages.append({\n",
        "        'role': 'user',\n",
        "        'content': prompt_to_classify\n",
        "    })\n",
        "\n",
        "    try:\n",
        "        # This will now connect to the 'ollama serve &' in Cell 3\n",
        "        response = ollama.chat(\n",
        "            model='phi3:mini',\n",
        "            messages=messages,\n",
        "            options={\n",
        "                'num_ctx': 1024,\n",
        "                'temperature': 0.0\n",
        "            }\n",
        "        )\n",
        "\n",
        "        classification = response['message']['content'].lower().strip()\n",
        "\n",
        "        words = re.split(r'[^a-z]+', classification)\n",
        "        first_word = words[0] if words else \"\"\n",
        "\n",
        "        if first_word in [\"low\", \"medium\", \"high\"]:\n",
        "            return first_word\n",
        "\n",
        "        print(f\"Warning: Model gave unexpected output: '{classification}'\", file=sys.stderr)\n",
        "        return \"medium\" # Default to medium if confused\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error during classification: {e}\", file=sys.stderr)\n",
        "        return \"error\"\n",
        "\n",
        "# ===================================================================\n",
        "# 2. THE ROUTER (The \"Engine\" Logic)\n",
        "# This is the new \"auto mode\" function you asked for.\n",
        "# ===================================================================\n",
        "\n",
        "# Define our \"model menu\" for the carbon-aware router\n",
        "MODEL_MENU = {\n",
        "    \"low\": \"gemma2:2b\",         # Smallest, most efficient model\n",
        "    \"medium\": \"llama3:8b\",    # Good, balanced local model\n",
        "    \"high\": \"gpt-4o-api\"      # \"Expensive\" API model\n",
        "}\n",
        "\n",
        "def auto_route_prompt(user_prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    This is the main \"auto mode\" engine.\n",
        "    1. It classifies the prompt.\n",
        "    2. It selects the right model from the menu.\n",
        "    \"\"\"\n",
        "    print(f\"Prompt: \\\"{user_prompt}\\\"\")\n",
        "\n",
        "    # --- Step 1: Classify (using our 'phi3:mini' brain) ---\n",
        "    complexity = predict_complexity(user_prompt)\n",
        "\n",
        "    if complexity == \"error\":\n",
        "        print(\"   >>> ERROR: Could not classify. Defaulting to medium.\")\n",
        "        return MODEL_MENU[\"medium\"]\n",
        "\n",
        "    print(f\"   - Classified as: {complexity.upper()}\")\n",
        "\n",
        "    # --- Step 2: Route (the \"auto mode\" selection) ---\n",
        "    selected_model = MODEL_MENU[complexity]\n",
        "    print(f\"   - Selected model: {selected_model}\")\n",
        "\n",
        "    # --- Step 3: Carbon-Logic (Your final goal) ---\n",
        "    # This is where you will add your carbon-check\n",
        "    if complexity == \"high\":\n",
        "        # grid_is_clean = check_carbon_api() # <--- Your future function\n",
        "        # if not grid_is_clean:\n",
        "        #    print(\"   - Carbon grid is 'dirty', falling back to medium model.\")\n",
        "        #    selected_model = MODEL_MENU[\"medium\"]\n",
        "        pass # Placeholder for your carbon logic\n",
        "\n",
        "    return selected_model\n",
        "\n",
        "# ===================================================================\n",
        "# 3. RUN THE ENGINE (Demo)\n",
        "# ===================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    prompts_to_test = [\n",
        "        \"What is 1 + 1?\",\n",
        "        \"Explain quantum entanglement in simple terms.\",\n",
        "        \"Compare the ethical implications of AI in healthcare vs autonomous vehicles.\",\n",
        "        \"explain cholorophyll\"\n",
        "    ]\n",
        "\n",
        "    print(f\"--- Running Auto-Mode Selection Engine ---\")\n",
        "\n",
        "    for p in prompts_to_test:\n",
        "\n",
        "        # This one function does it all\n",
        "        selected_model = auto_route_prompt(p)\n",
        "\n",
        "        # --- MOCK EXECUTION ---\n",
        "        # This is where you would *actually* run the final prompt\n",
        "        print(f\"   >>> [MOCK] Would run: ollama.chat(model='{selected_model}', ...)\")\n",
        "        print(\"-\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNMOvhjm_g0u",
        "outputId": "965bd3d4-c327-4df4-decb-de2cb36688e2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running Auto-Mode Selection Engine ---\n",
            "Prompt: \"What is 1 + 1?\"\n",
            "   - Classified as: LOW\n",
            "   - Selected model: gemma2:2b\n",
            "   >>> [MOCK] Would run: ollama.chat(model='gemma2:2b', ...)\n",
            "--------------------\n",
            "Prompt: \"Explain quantum entanglement in simple terms.\"\n",
            "   - Classified as: MEDIUM\n",
            "   - Selected model: llama3:8b\n",
            "   >>> [MOCK] Would run: ollama.chat(model='llama3:8b', ...)\n",
            "--------------------\n",
            "Prompt: \"Compare the ethical implications of AI in healthcare vs autonomous vehicles.\"\n",
            "   - Classified as: MEDIUM\n",
            "   - Selected model: llama3:8b\n",
            "   >>> [MOCK] Would run: ollama.chat(model='llama3:8b', ...)\n",
            "--------------------\n",
            "Prompt: \"explain cholorophyll\"\n",
            "   - Classified as: LOW\n",
            "   - Selected model: gemma2:2b\n",
            "   >>> [MOCK] Would run: ollama.chat(model='gemma2:2b', ...)\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: Model gave unexpected output: 'quantum entanglement is when two particles become linked, and the state of one instantly influences the other, no matter how far apart they are. it's like having a pair of dice that always land on complementary numbers at once; if you roll them separately in different rooms, their results will still match perfectly without any direct interaction between them. this strange connection defies our everyday understanding and even einstein called it \"spooky action at a distance.\"'\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMLSqyY0LSwnE/eYIK60V61",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}